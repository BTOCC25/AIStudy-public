{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06d6b92d-fabe-4aeb-9238-5fb0765909a1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Top_Down faster-rcnn scratch code review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180f9bf7-c94e-40ca-9b24-39bb445ddbbb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2180deb7-c6db-40ad-873e-a6644a3b7551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "# default\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset \n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# torchvision\n",
    "from torchvision.models import vgg16\n",
    "from torchvision.ops import RoIPool\n",
    "from torchvision.ops import nms\n",
    "\n",
    "# image load\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# augumentation\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# util\n",
    "from pathlib import Path # Path Í∞ùÏ≤¥\n",
    "from pycocotools.coco import COCO # COCOÌòïÏãù JSON ÌååÏùº ÏÜêÏâΩÍ≤å ÏùΩÍ∏∞\n",
    "from torchsummary import summary # model summary\n",
    "from collections import namedtuple # tuple ÏÉùÏÑ±\n",
    "\n",
    "from torchnet.meter import ConfusionMeter, AverageValueMeter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2181bb57-8f17-42b9-a0ff-223cc3ca1dc7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# hyper-parameter set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "162fce00-f68f-40a8-8422-cb9f3e0c9436",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('../../detection/dataset') # Îç∞Ïù¥ÌÑ∞ Í≤ΩÎ°ú\n",
    "use_drop = False   # use dropout in RoIHead\n",
    "\n",
    "epochs=1\n",
    "learning_rate = 1e-3\n",
    "lr_decay = 0.1\n",
    "weight_decay = 0.0005\n",
    "\n",
    "train_load_path = None  # trainÏãú checkpoint Í≤ΩÎ°ú\n",
    "inf_load_path = './checkpoints/faster_rcnn_scratch_checkpoints.pth' # inferenceÏãú Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Í≤ΩÎ°ú"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71be1f6c-244b-4fcd-bc60-ef4956972d37",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Util Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "04946f0f-9c55-41db-b822-ee3e3197fb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loc2bbox(src_bbox, loc):\n",
    "    \"\"\"\n",
    "    from src_bbox to dst bbox using loc\n",
    "    Args:\n",
    "        src_bbox: ÏÜåÏä§ Î∞îÏö¥Îî© Î∞ïÏä§\n",
    "        loc: Îç∏ÌÉÄ\n",
    "    Returns: dst_bbox\n",
    "    \"\"\"\n",
    "\n",
    "    if src_bbox.shape[0] == 0:\n",
    "        return np.zeros((0, 4), dtype=loc.dtype)\n",
    "\n",
    "    src_bbox = src_bbox.astype(src_bbox.dtype, copy=False)\n",
    "\n",
    "    # x_min, y_min, x_max, y_max\n",
    "    src_height = src_bbox[:, 2] - src_bbox[:, 0]\n",
    "    src_width = src_bbox[:, 3] - src_bbox[:, 1]\n",
    "    src_ctr_y = src_bbox[:, 0] + 0.5 * src_height\n",
    "    src_ctr_x = src_bbox[:, 1] + 0.5 * src_width\n",
    "\n",
    "    dy = loc[:, 0::4]\n",
    "    dx = loc[:, 1::4]\n",
    "    dh = loc[:, 2::4]\n",
    "    dw = loc[:, 3::4]\n",
    "\n",
    "    ctr_y = dy * src_height[:, np.newaxis] + src_ctr_y[:, np.newaxis]\n",
    "    ctr_x = dx * src_width[:, np.newaxis] + src_ctr_x[:, np.newaxis]\n",
    "    h = np.exp(dh) * src_height[:, np.newaxis]\n",
    "    w = np.exp(dw) * src_width[:, np.newaxis]\n",
    "\n",
    "    dst_bbox = np.zeros(loc.shape, dtype=loc.dtype)\n",
    "    dst_bbox[:, 0::4] = ctr_y - 0.5 * h\n",
    "    dst_bbox[:, 1::4] = ctr_x - 0.5 * w\n",
    "    dst_bbox[:, 2::4] = ctr_y + 0.5 * h\n",
    "    dst_bbox[:, 3::4] = ctr_x + 0.5 * w\n",
    "\n",
    "    return dst_bbox\n",
    "\n",
    "\n",
    "def bbox2loc(src_bbox, dst_bbox):\n",
    "    \"\"\"\n",
    "    src_bbox : ÏòàÏ∏°Îêú Ï¢åÌëúÍ∞í(or anchor), dst_bbox: gt Ï¢åÌëúÍ∞í -> loc(y, x, h, w)\n",
    "    \"\"\"\n",
    "\n",
    "    # x_min, y_min, x_max, y_max\n",
    "    height = src_bbox[:, 2] - src_bbox[:, 0]\n",
    "    width = src_bbox[:, 3] - src_bbox[:, 1]\n",
    "    ctr_y = src_bbox[:, 0] + 0.5 * height\n",
    "    ctr_x = src_bbox[:, 1] + 0.5 * width\n",
    "\n",
    "    # x_min, y_min, x_max, y_max\n",
    "    base_height = dst_bbox[:, 2] - dst_bbox[:, 0]\n",
    "    base_width = dst_bbox[:, 3] - dst_bbox[:, 1]\n",
    "    base_ctr_y = dst_bbox[:, 0] + 0.5 * base_height\n",
    "    base_ctr_x = dst_bbox[:, 1] + 0.5 * base_width\n",
    "\n",
    "    eps = np.finfo(height.dtype).eps\n",
    "    height = np.maximum(height, eps)\n",
    "    width = np.maximum(width, eps)\n",
    "\n",
    "    dy = (base_ctr_y - ctr_y) / height\n",
    "    dx = (base_ctr_x - ctr_x) / width\n",
    "    dh = np.log(base_height / height)\n",
    "    dw = np.log(base_width / width)\n",
    "\n",
    "    loc = np.vstack((dy, dx, dh, dw)).transpose()\n",
    "    return loc\n",
    "\n",
    "\n",
    "def normal_init(m, mean, stddev, truncated=False):\n",
    "    \"\"\"\n",
    "    weight initialization\n",
    "    \"\"\"\n",
    "    if truncated:\n",
    "        m.weight.data.normal_().fmod_(2).mul_(stddev).add_(mean) \n",
    "    else:\n",
    "        m.weight.data.normal_(mean, stddev)\n",
    "        m.bias.data.zero_()\n",
    "\n",
    "\n",
    "def get_inside_index(anchor, H, W):\n",
    "    # Calc indicies of anchors which are located completely inside of the image\n",
    "    # whose size is speficied.\n",
    "    index_inside = np.where(\n",
    "        (anchor[:, 0] >= 0) &\n",
    "        (anchor[:, 1] >= 0) &\n",
    "        (anchor[:, 2] <= H) &\n",
    "        (anchor[:, 3] <= W)\n",
    "    )[0]\n",
    "    return index_inside\n",
    "\n",
    "\n",
    "def unmap(data, count, index, fill=0):\n",
    "    # Unmap a subset of item (data) back to the original set of items (of size count)\n",
    "    if len(data.shape) == 1:\n",
    "        ret = np.empty((count,), dtype=data.dtype)\n",
    "        ret.fill(fill)\n",
    "        ret[index] = data\n",
    "    else:\n",
    "        ret = np.empty((count,) + data.shape[1:], dtype=data.dtype)\n",
    "        ret.fill(fill)\n",
    "        ret[index, :] = data\n",
    "    return ret\n",
    "\n",
    "\n",
    "## util ##\n",
    "def tonumpy(data):\n",
    "    if isinstance(data, np.ndarray):\n",
    "        return data\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        return data.detach().cpu().numpy()\n",
    "\n",
    "def totensor(data, cuda = True):\n",
    "    if isinstance(data, np.ndarray):\n",
    "        tensor = torch.from_numpy(data)\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        tensor = data.detach()\n",
    "    if cuda:\n",
    "        tensor = tensor.cuda()\n",
    "    return tensor\n",
    "\n",
    "def scalar(data):\n",
    "    if isinstance(data, np.ndarray):\n",
    "        return data.reshape(1)[0]\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        return data.item()\n",
    "    \n",
    "def bbox_iou(bbox_a, bbox_b):\n",
    "    if bbox_a.shape[1] != 4 or bbox_b.shape[1] != 4:\n",
    "        raise IndexError\n",
    "\n",
    "    #bbox_a 1Í∞úÏôÄ bbox_b kÍ∞úÎ•º ÎπÑÍµêÌï¥ÏïºÌïòÎØÄÎ°ú NoneÏùÑ Ïù¥Ïö©Ìï¥ÏÑú Ï∞®ÏõêÏùÑ ÎäòÎ†§ÏÑú Ïó∞ÏÇ∞ÌïúÎã§.\n",
    "    # top left\n",
    "    tl = np.maximum(bbox_a[:, None, :2], bbox_b[:, :2])\n",
    "    # bottom right\n",
    "    br = np.minimum(bbox_a[:, None, 2:], bbox_b[:, 2:])\n",
    "\n",
    "    area_i = np.prod(br - tl, axis=2) * (tl < br).all(axis=2)\n",
    "    area_a = np.prod(bbox_a[:, 2:] - bbox_a[:, :2], axis=1)\n",
    "    area_b = np.prod(bbox_b[:, 2:] - bbox_b[:, :2], axis=1)\n",
    "    return area_i / (area_a[:, None] + area_b - area_i)\n",
    "\n",
    "def nograd(f):\n",
    "    def new_f(*args, **kwargs):\n",
    "        with torch.no_grad():\n",
    "            return f(*args, **kwargs)\n",
    "    return new_f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d5c169-9bcc-4479-9ddf-f14805a772d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d279ce-514d-4024-9cc5-40bfd841022b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1-0. code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b6f48d29-9f56-4c5e-bbfc-c9eadeee872f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    data_dir = Path('../../detection/dataset')\n",
    "    annotation = data_dir / 'train.json'\n",
    "    dataset = CustomDataset(annotation, data_dir, transforms=get_train_transform())\n",
    "    \n",
    "    print('-' * 50)\n",
    "    print('load data')\n",
    "    print(f'image shape : {dataset[0][0].shape}')\n",
    "    dataloader = DataLoader(dataset, \n",
    "                            batch_size=1,     # only batch_size=1 support\n",
    "                            shuffle=True, \n",
    "                            pin_memory=False,\n",
    "                            num_workers=0)\n",
    "    \n",
    "    # faster rcnn trainer Î∂àÎü¨Ïò§Í∏∞\n",
    "    faster_rcnn = FasterRCNNVGG16().cuda()\n",
    "    trainer = FasterRCNNTrainer(faster_rcnn).cuda()\n",
    "    \n",
    "    # checkpoint load\n",
    "    # if train_load_path:\n",
    "    #     trainer.load(train_load_path)\n",
    "    #     print('load pretrained model from %s' % train_load_path)\n",
    "    # lr_ = learning_rate\n",
    "    # best_loss = 1000\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        trainer.reset_meters() # üß°\n",
    "        for ii, (img, bbox_, label_, scale) in enumerate(tqdm(dataloader)):\n",
    "            img, bbox, label = img.cuda().float(), bbox_.cuda(), label_.cuda()\n",
    "            \n",
    "             # üíö batch size 1Î°ú Í≥†Ï†ï\n",
    "            # print(img.shape, bbox.shape, label.shape) \n",
    "            # torch.Size([1, 3, 800, 800]) torch.Size([1, 1, 4]) torch.Size([1, 1])\n",
    "            \n",
    "            trainer.train_step(img, bbox, label, float(scale))\n",
    "            break\n",
    "    \n",
    "        losses = trainer.get_meter_data()\n",
    "        print(f\"Epoch #{epoch+1} loss: {losses}\")\n",
    "        if losses['total_loss'] < best_loss :\n",
    "            trainer.save()\n",
    "            \n",
    "        if epoch == 9:\n",
    "            trainer.faster_rcnn.scale_lr(lr_decay)\n",
    "            lr_ = lr_ * lr_decay\n",
    "\n",
    "        if epoch == 13: \n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda6a77f-c4af-46fd-aa28-0bced60589cc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1-1. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f7be5ca4-c612-40a2-b543-ad65b9c62ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, annotation, data_dir, transforms):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.coco = COCO(annotation) # Experiment 1-1\n",
    "        self.transforms = transforms \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # Ïù¥ÎØ∏ÏßÄ Ï†ïÎ≥¥ Î∂àÎü¨Ïò§Í∏∞\n",
    "        image_id = self.coco.getImgIds(imgIds=index)\n",
    "        image_info = self.coco.loadImgs(image_id)[0]\n",
    "\n",
    "        # Ïù¥ÎØ∏ÏßÄ Î°úÎìú \n",
    "        image = cv2.imread(str(Path(self.data_dir) / image_info['file_name']))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "\n",
    "        # Ïñ¥ÎÖ∏ÌÖåÏù¥ÏÖò Ï†ïÎ≥¥ Î°úÎìú \n",
    "        ann_ids = self.coco.getAnnIds(imgIds=image_info['id'])\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "        # Î∞ïÏä§ Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        boxes = np.array([x['bbox'] for x in anns])\n",
    "\n",
    "        #-- x, y, w, h -> x1, y1, x2, y2(coco -> pascal_voc)\n",
    "        # boxes (x_min, y_min, x_max, y_max)\n",
    "        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n",
    "        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n",
    "\n",
    "        # Î†àÏù¥Î∏î Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        labels = np.array([x['category_id'] for x in anns])\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        \n",
    "        # transform\n",
    "        sample = {\n",
    "            'image': image,\n",
    "            'bboxes': boxes,\n",
    "            'labels': labels\n",
    "        }\n",
    "        sample = self.transforms(**sample)\n",
    "        image = sample['image']\n",
    "        bboxes = torch.tensor(sample['bboxes'], dtype=torch.float32)\n",
    "        boxes = torch.tensor(sample['bboxes'], dtype=torch.float32)\n",
    "\n",
    "        # bboxes (x_min, y_min, x_max, y_max) -> boxes (y_min, x_min, y_max, x_max)\n",
    "        boxes[:, 0] = bboxes[:, 1]\n",
    "        boxes[:, 1] = bboxes[:, 0]\n",
    "        boxes[:, 2] = bboxes[:, 3]\n",
    "        boxes[:, 3] = bboxes[:, 2]\n",
    "\n",
    "        return image, boxes, labels, 1.0\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.coco.getImgIds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85091a69-b14f-4d27-8a9b-281d9a4cee06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset transform\n",
    "def get_train_transform():\n",
    "    return A.Compose([\n",
    "        A.Resize(height = 800, width = 800),\n",
    "        A.Flip(p=0.5),\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "\n",
    "# No transform\n",
    "def no_transform():\n",
    "    return A.Compose([\n",
    "        ToTensorV2(p=1.0) # format for pytorch tensor\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a852052e-367b-4b8c-b767-f997c431e489",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1-2) Faster-RCNN Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3497abd6-ae6c-4753-a4d4-36b1488262bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1-2-0) code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0f501f7b-b4fb-4b89-8bc5-b7aa389c449a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LossTuple = namedtuple('LossTuple', ['rpn_loc_loss', 'rpn_cls_loss',\n",
    "                                     'roi_loc_loss', 'roi_cls_loss',\n",
    "                                     'total_loss'])\n",
    "class FasterRCNNTrainer(nn.Module):\n",
    "    def __init__(self, faster_rcnn):\n",
    "        super(FasterRCNNTrainer, self).__init__()\n",
    "\n",
    "        self.faster_rcnn = faster_rcnn\n",
    "        self.rpn_sigma = 3.     # sigma for l1_smooth_loss (RPN loss)\n",
    "        self.roi_sigma = 1.     # sigma for l1_smooth_loss (ROI loss)\n",
    "\n",
    "        # target creator create gt_bbox gt_label etc as training targets. \n",
    "        self.anchor_target_creator = AnchorTargetCreator()\n",
    "        self.proposal_target_creator = ProposalTargetCreator()\n",
    "\n",
    "        self.loc_normalize_mean = faster_rcnn.loc_normalize_mean\n",
    "        self.loc_normalize_std = faster_rcnn.loc_normalize_std\n",
    "        self.optimizer = self.faster_rcnn.get_optimizer()\n",
    "\n",
    "        # training ÏÉÅÌÉú Î≥¥Ïó¨Ï£ºÎäî ÏßÄÌëú\n",
    "        self.rpn_cm = ConfusionMeter(2) # confusion matrix for classification\n",
    "        self.roi_cm = ConfusionMeter(11)  # confusion matrix for classification\n",
    "        self.meters = {k: AverageValueMeter() for k in LossTuple._fields}  # average loss\n",
    "\n",
    "    def forward(self, imgs, bboxes, labels, scale):\n",
    "        # üíö torch.Size([1, 3, 800, 800]) torch.Size([1, 1, 4]) torch.Size([1, 1])\n",
    "        n = bboxes.shape[0]\n",
    "        \n",
    "        if n != 1:\n",
    "            raise ValueError('Currently only batch size 1 is supported.')\n",
    "\n",
    "        _, _, H, W = imgs.shape\n",
    "        # üíö (800, 800)\n",
    "        img_size = (H, W)\n",
    "        \n",
    "        # VGG (features extractor)\n",
    "        # torch.Size([1, 512, 50, 50])\n",
    "        features = self.faster_rcnn.extractor(imgs)\n",
    "        \n",
    "        # RPN (region proposal)\n",
    "        # üíö input :  torch.Size([1, 512, 50, 50]), (800, 800), 1\n",
    "        # üíö output : torch.size([1, 22500, 4]), torch.size([1, 22500, 2]), (2000, 4) (2000,), (22500, 4)\n",
    "        rpn_locs, rpn_scores, rois, roi_indices, anchor = self.faster_rcnn.rpn(features, img_size, scale)\n",
    "\n",
    "        # Since batch size is one, convert variables to singular form\n",
    "        bbox = bboxes[0] # batch 1\n",
    "        label = labels[0] # batch 1\n",
    "        rpn_score = rpn_scores[0]\n",
    "        rpn_loc = rpn_locs[0]\n",
    "        roi = rois\n",
    "        \n",
    "        \"\"\"\n",
    "        sample roi =  rpnÏóêÏÑú nms Í±∞Ïπú 2000Í∞úÏùò roiÎì§ Ï§ë positive/negative ÎπÑÏú® Í≥†Î†§Ìï¥ ÏµúÏ¢Ö samplingÌïú roi\n",
    "        \"\"\"\n",
    "        #  üíö input : (2000, 4), (boxÏàò, 4), (boxÏàò, 1), (0., 0., 0., 0.), (0.1, 0.1, 0.2, 0.2))\n",
    "        sample_roi, gt_roi_loc, gt_roi_label = self.proposal_target_creator(\n",
    "            roi,\n",
    "            tonumpy(bbox),\n",
    "            tonumpy(label),\n",
    "            self.loc_normalize_mean,\n",
    "            self.loc_normalize_std)\n",
    "        \n",
    "        print(sample_roi.shape, gt_roi_loc.shape, gt_roi_label.shape)\n",
    "        # NOTE it's all zero because now it only support for batch=1 now\n",
    "        # Faster R-CNN head (prediction head)\n",
    "        sample_roi_index = torch.zeros(len(sample_roi))\n",
    "        roi_cls_loc, roi_score = self.faster_rcnn.head(features,sample_roi,sample_roi_index) \n",
    "\n",
    "        # ------------------ RPN losses -------------------#\n",
    "        gt_rpn_loc, gt_rpn_label = self.anchor_target_creator(tonumpy(bbox),anchor,img_size) \n",
    "        gt_rpn_label = totensor(gt_rpn_label).long() \n",
    "        gt_rpn_loc = totensor(gt_rpn_loc) \n",
    "        \n",
    "        # rpn bounding box regression loss\n",
    "        rpn_loc_loss = _fast_rcnn_loc_loss(rpn_loc,gt_rpn_loc,gt_rpn_label.data,self.rpn_sigma)\n",
    "        # rpn classification loss\n",
    "        rpn_cls_loss = F.cross_entropy(rpn_score, gt_rpn_label.cuda(), ignore_index=-1)\n",
    "        \n",
    "        _gt_rpn_label = gt_rpn_label[gt_rpn_label > -1]\n",
    "        _rpn_score = tonumpy(rpn_score)[tonumpy(gt_rpn_label) > -1]\n",
    "        self.rpn_cm.add(totensor(_rpn_score, False), _gt_rpn_label.data.long())\n",
    "\n",
    "        # ------------------ ROI losses (fast rcnn loss) -------------------#\n",
    "        n_sample = roi_cls_loc.shape[0] \n",
    "        roi_cls_loc = roi_cls_loc.view(n_sample, -1, 4)\n",
    "        roi_loc = roi_cls_loc[torch.arange(0, n_sample).long().cuda(), \\\n",
    "                              totensor(gt_roi_label).long()]\n",
    "        gt_roi_label = totensor(gt_roi_label).long() \n",
    "        gt_roi_loc = totensor(gt_roi_loc) \n",
    "\n",
    "        # faster rcnn bounding box regression loss\n",
    "        roi_loc_loss = _fast_rcnn_loc_loss(\n",
    "            roi_loc.contiguous(),\n",
    "            gt_roi_loc,\n",
    "            gt_roi_label.data,\n",
    "            self.roi_sigma)\n",
    "\n",
    "        # faster rcnn classification loss\n",
    "        roi_cls_loss = nn.CrossEntropyLoss()(roi_score, gt_roi_label.cuda())\n",
    "        \n",
    "        self.roi_cm.add(totensor(roi_score, False), gt_roi_label.data.long())\n",
    "\n",
    "        losses = [rpn_loc_loss, rpn_cls_loss, roi_loc_loss, roi_cls_loss]\n",
    "        losses = losses + [sum(losses)] # total_loss == sum(losses)\n",
    "\n",
    "        return LossTuple(*losses)\n",
    "    \n",
    "    # training\n",
    "    def train_step(self, imgs, bboxes, labels, scale):\n",
    "        \n",
    "        # üíö torch.Size([1, 3, 800, 800]) torch.Size([1, 1, 4]) torch.Size([1, 1])\n",
    "        self.optimizer.zero_grad()\n",
    "        losses = self.forward(imgs, bboxes, labels, scale)\n",
    "        losses.total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.update_meters(losses)\n",
    "        return losses\n",
    "    \n",
    "    # checkpoint ÎßåÎì§Í∏∞\n",
    "    def save(self, save_optimizer=False, save_path=None):\n",
    "        save_dict = dict()\n",
    "\n",
    "        save_dict['model'] = self.faster_rcnn.state_dict()\n",
    "\n",
    "        if save_optimizer:\n",
    "            save_dict['optimizer'] = self.optimizer.state_dict()\n",
    "\n",
    "        if save_path is None:\n",
    "            save_path = './checkpoints/faster_rcnn_scratch_checkpoints.pth'\n",
    "\n",
    "        save_dir = os.path.dirname(save_path)\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "\n",
    "        torch.save(save_dict, save_path)\n",
    "        return save_path\n",
    "    \n",
    "    # checkpoint load\n",
    "    def load(self, path, load_optimizer=True, parse_opt=False, ):\n",
    "        state_dict = torch.load(path)\n",
    "        if 'model' in state_dict:\n",
    "            self.faster_rcnn.load_state_dict(state_dict['model'])\n",
    "        else:  # legacy way, for backward compatibility\n",
    "            self.faster_rcnn.load_state_dict(state_dict)\n",
    "            return self\n",
    "        if 'optimizer' in state_dict and load_optimizer:\n",
    "            self.optimizer.load_state_dict(state_dict['optimizer'])\n",
    "        return self\n",
    "\n",
    "    def update_meters(self, losses):\n",
    "        loss_d = {k: scalar(v) for k, v in losses._asdict().items()}\n",
    "        for key, meter in self.meters.items():\n",
    "            meter.add(loss_d[key])\n",
    "\n",
    "    def reset_meters(self):\n",
    "        for key, meter in self.meters.items():\n",
    "            meter.reset()\n",
    "        self.roi_cm.reset()\n",
    "        self.rpn_cm.reset()\n",
    "\n",
    "    def get_meter_data(self):\n",
    "        return {k: v.value()[0] for k, v in self.meters.items()}\n",
    "\n",
    "\n",
    "def _smooth_l1_loss(x, t, in_weight, sigma):\n",
    "    sigma2 = sigma ** 2\n",
    "    diff = in_weight * (x - t)\n",
    "    abs_diff = diff.abs()\n",
    "    flag = (abs_diff.data < (1. / sigma2)).float()\n",
    "    y = (flag * (sigma2 / 2.) * (diff ** 2) +\n",
    "         (1 - flag) * (abs_diff - 0.5 / sigma2))\n",
    "    return y.sum()\n",
    "\n",
    "\n",
    "def _fast_rcnn_loc_loss(pred_loc, gt_loc, gt_label, sigma):\n",
    "    # Localization loss Íµ¨Ìï† ÎïåÎäî positive exampleÏóê ÎåÄÌï¥ÏÑúÎßå Í≥ÑÏÇ∞\n",
    "    in_weight = torch.zeros(gt_loc.shape).cuda()\n",
    "    in_weight[(gt_label > 0).view(-1, 1).expand_as(in_weight).cuda()] = 1\n",
    "    loc_loss = _smooth_l1_loss(pred_loc, gt_loc, in_weight.detach(), sigma)\n",
    "    loc_loss /= ((gt_label >= 0).sum().float())\n",
    "    return loc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f4304937-cfa9-4bdd-87ac-38b80f72c8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.07s)\n",
      "creating index...\n",
      "index created!\n",
      "--------------------------------------------------\n",
      "load data\n",
      "image shape : torch.Size([3, 800, 800])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4883 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 4) (128, 4) (128,)\n",
      "check\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb14a55-788e-46b9-8175-b5de2f55b16f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1-2-1) FasterRCNNVGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e145b7eb-2e9e-4ae9-bc8d-fda6bbb6afdf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 1-2-1-0) code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9772b43e-9007-459c-80d3-084e81b6b0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FasterRCNNVGG16(FasterRCNN):\n",
    "\n",
    "    feat_stride = 16  # downsample 16x for output of conv5 in vgg16\n",
    "\n",
    "    def __init__(self, n_fg_class=10, ratios=[0.5, 1, 2], anchor_scales=[8, 16, 32] ): # n_fg_class : Î∞∞Í≤ΩÌè¨Ìï® ÌïòÏßÄ ÏïäÏùÄ class Í∞úÏàò        \n",
    "        extractor, classifier = decom_vgg16(pretrained=True)\n",
    "        \n",
    "        rpn = RegionProposalNetwork(\n",
    "            512, 512,\n",
    "            ratios=ratios,\n",
    "            anchor_scales=anchor_scales,\n",
    "            feat_stride=self.feat_stride,\n",
    "        )\n",
    "\n",
    "        head = VGG16RoIHead(\n",
    "            n_class=n_fg_class + 1,\n",
    "            roi_size=7,\n",
    "            spatial_scale=(1. / self.feat_stride),\n",
    "            classifier=classifier\n",
    "        )\n",
    "        super(FasterRCNNVGG16, self).__init__(\n",
    "            extractor,\n",
    "            rpn,\n",
    "            head,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0de799-f649-49db-a556-546117901a1a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 1-2-1-1) extractor, classifier (backborn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50aad2bb-d79e-460e-9ea7-29f2ad45641c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decom_vgg16(pretrained: bool):\n",
    "    # the 30th layer of features is relu of conv5_3\n",
    "    model = vgg16(pretrained=pretrained)\n",
    "    \n",
    "    features = list(model.features)[:30] # max_poll Ï†úÏô∏\n",
    "    classifier = model.classifier\n",
    "\n",
    "    classifier = list(classifier)\n",
    "    del classifier[6]\n",
    "    if not use_drop:\n",
    "        del classifier[5]\n",
    "        del classifier[2]\n",
    "    classifier = nn.Sequential(*classifier)\n",
    "\n",
    "    # freeze top4 conv\n",
    "    for layer in features[:10]:\n",
    "        for p in layer.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    return nn.Sequential(*features), classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce05f59b-10ce-4248-a140-f1065d294994",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 1-2-1-2) rpn(RegionProposalNetwork)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cfa3b2-32ef-4a8b-948a-e7345d0d5fd7",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 1-2-1-2-0) code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f10d1daf-9f28-4fc5-a16c-f63d92b8b0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegionProposalNetwork(nn.Module):\n",
    "    def __init__(self, in_channels=512, mid_channels=512, ratios=[0.5, 1, 2],\n",
    "                 anchor_scales=[8, 16, 32], feat_stride=16, proposal_creator_params=dict(),):\n",
    "        \"\"\" \n",
    "        vggÎ•º ÌÜµÍ≥ºÌïú feature mapÏóêÏÑú Í∞ÅÍ∞Å anchor box ÏÉùÏÑ±\n",
    "        Í∞Å anchor boxÎ≥ÑÎ°ú scoreÎ•º Í≥ÑÏÇ∞Ìï¥ÏïºÌïúÎã§.\n",
    "        base anchor boxÏùò ÏúÑÏπòÎ•º Ïñ¥ÎäêÎßåÌÅº Î∞îÍøÄÏßÄ ÎØ∏ÏÑ∏Ï°∞Ï†ïÌï¥ÏïºÌïúÎã§.\n",
    "        \n",
    "        Args:\n",
    "            ratios: ÎπÑÏú®\n",
    "            anchor_scales: Ïä§ÏºÄÏùº\n",
    "        Returns: \n",
    "        \"\"\"\n",
    "        super(RegionProposalNetwork, self).__init__()\n",
    "\n",
    "        self.anchor_base = generate_anchor_base(anchor_scales=anchor_scales, ratios=ratios) # 9Í∞úÏùò anchorbox ÏÉùÏÑ± (9 x 4)\n",
    "        self.feat_stride = feat_stride  \n",
    "        self.proposal_layer = ProposalCreator(self, **proposal_creator_params)\n",
    "        n_anchor = self.anchor_base.shape[0] # anchor Í∞úÏàò - 9\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=mid_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.score = nn.Conv2d(mid_channels, n_anchor * 2, 1, 1, 0)  # 9*2 (anchor box Ïàò x 2(object true, false))\n",
    "        self.loc = nn.Conv2d(mid_channels, n_anchor * 4, 1, 1, 0)   # 9*4 (anchor box Ïàò x (x, y, w, h) Ï°∞Ï†à Í∞í)\n",
    "        \n",
    "        # 0. utilÏóê Ï†ïÏùò Ï¥àÍ∏∞ÌôîÌïòÎäîÍ±∞\n",
    "        normal_init(self.conv1, 0, 0.01) # weight initalizer\n",
    "        normal_init(self.score, 0, 0.01) # weight initalizer\n",
    "        normal_init(self.loc, 0, 0.01)   # weight initalizer\n",
    "\n",
    "    def forward(self, x, img_size, scale=1.):\n",
    "        # input : üíö torch.Size([1, 512, 50, 50]), (800, 800), 1\n",
    "        n, _, hh, ww = x.shape\n",
    "\n",
    "        # Ï†ÑÏ≤¥ (h*w*9)Í∞ú anchorÏùò Ï¢åÌëúÍ∞í # anchor_base:(9, 4)\n",
    "        # ÌîΩÏÖÄÎãπ Î™®Îì† anchor box 9Í∞ú ÏÉùÏÑ±\n",
    "        # output = (22500, 4)\n",
    "        anchor = _enumerate_shifted_anchor(self.anchor_base, self.feat_stride, hh, ww) # torch.size([22500, 4])\n",
    "        n_anchor = self.anchor_base.shape[0]\n",
    "        \n",
    "        # middle ÌïúÎ≤à layer ÌÜµÍ≥º\n",
    "        # üíötorch.size([1, 512, 50, 50]) -> torch.Size([1, 512, 50, 50])\n",
    "        middle = F.relu(self.conv1(x))\n",
    "        \n",
    "        # predicted bounding box offset\n",
    "        # bounding_box Î≥¥Ï†ï üíö torch.Size([1, 512, 50, 50])-> torch.size([1, 36, 50, 50])\n",
    "        rpn_locs = self.loc(middle) \n",
    "        # torch.size([1, 22500, 4]) # anchorÎûë ÎßûÏ∂∞Ï§å\n",
    "        rpn_locs = rpn_locs.permute(0, 2, 3, 1).contiguous().view(n, -1, 4)\n",
    "\n",
    "        # predicted scores for anchor (foreground or background)\n",
    "        # üíö torch.Size([1, 512, 50, 50]) -> torch.size([1, 18, 50, 50])\n",
    "        rpn_scores = self.score(middle)  \n",
    "        # torch.size([1, 50, 50, 18])\n",
    "        rpn_scores = rpn_scores.permute(0, 2, 3, 1).contiguous() \n",
    "        \n",
    "        # scores for foreground\n",
    "        rpn_softmax_scores = F.softmax(rpn_scores.view(n, hh, ww, n_anchor, 2), dim=4) # scoreÎ°ú softmax torch.size([1, 50, 50, 9, 2])\n",
    "        rpn_fg_scores = rpn_softmax_scores[:, :, :, :, 1].contiguous() # torch.size([1, 50, 50, 9]) # objectÍ∞Ä Ï°¥Ïû¨ÌïòÎäîÍ±∞ Í∞ÄÏ†∏Ïò¥\n",
    "        rpn_fg_scores = rpn_fg_scores.view(n, -1) # torch.size([1, 22500])\n",
    "        \n",
    "        rpn_scores = rpn_scores.view(n, -1, 2) # torch.size([n, 50, 50, 18]) -> torch.size([1, 22500, 2]) # anchorÎûë ÎßûÏ∂∞Ï§å\n",
    "\n",
    "        # proposalÏÉùÏÑ± (ProposalCreator)\n",
    "        # 22500 -> 2000Í∞ú ÎΩëÏûê\n",
    "        rois = list()        # proposalÏùò Ï¢åÌëúÍ∞íÏù¥ ÏûàÎäî bounding box array\n",
    "        roi_indices = list() # roiÏóê Ìï¥ÎãπÌïòÎäî image Ïù∏Îç±Ïä§\n",
    "        for i in range(n):\n",
    "            # (22500, 4) loc Ï†ïÎ≥¥, (22500,)Ïùò  fg score, (22500, 4) anchore box, ÏõêÎ≥∏Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞, scale Í∞í = 1\n",
    "            # return (2000, 4)\n",
    "            roi = self.proposal_layer(rpn_locs[i].cpu().data.numpy(),rpn_fg_scores[i].cpu().data.numpy(), anchor, img_size, scale=scale)\n",
    "            \n",
    "            batch_index = i * np.ones((len(roi),), dtype=np.int32)\n",
    "            rois.append(roi)\n",
    "            roi_indices.append(batch_index)\n",
    "        rois = np.concatenate(rois, axis=0)\n",
    "        roi_indices = np.concatenate(roi_indices, axis=0)\n",
    "        \n",
    "        # torch.size([1, 22500, 4]), torch.size([1, 22500, 2]), (2000, 4) (2000,), (22500, 4)\n",
    "        return rpn_locs, rpn_scores, rois, roi_indices, anchor\n",
    "\n",
    "\n",
    "def _enumerate_shifted_anchor(anchor_base, feat_stride, height, width):\n",
    "    # -- ÏõêÎ≥∏Ïù¥ÎØ∏ÏßÄ Í∏∞Ï§Ä\n",
    "\n",
    "    # anchor_baseÎäî ÌïòÎÇòÏùò pixelÏóê 9Í∞ú Ï¢ÖÎ•òÏùò anchor boxÎ•º ÎÇòÌÉÄÎÉÑ\n",
    "    # Ïù¥Í≤ÉÏùÑ enumerateÏãúÏºú Ï†ÑÏ≤¥ Ïù¥ÎØ∏ÏßÄÏùò pixelÏóê Í∞ÅÍ∞Å 9Í∞úÏùò anchor boxÎ•º Í∞ÄÏßÄÍ≤å Ìï®\n",
    "    # 32x32 feature mapÏóêÏÑúÎäî 32x32x9=9216Í∞ú anchor boxÍ∞ÄÏßê\n",
    "\n",
    "    shift_y = np.arange(0, height * feat_stride, feat_stride) # (1 x height)\n",
    "    shift_x = np.arange(0, width * feat_stride, feat_stride) # (1 x width)\n",
    "    shift_x, shift_y = np.meshgrid(shift_x, shift_y) # (width, height)\n",
    "    shift = np.stack((shift_y.ravel(), shift_x.ravel(),\n",
    "                      shift_y.ravel(), shift_x.ravel()), axis=1) # ((width x height), 4)\n",
    "\n",
    "    A = anchor_base.shape[0] # anchor Ïàò\n",
    "    K = shift.shape[0] # Ï¥ù pixel Ïàò\n",
    "\n",
    "    anchor = anchor_base.reshape((1, A, 4)) + \\\n",
    "             shift.reshape((1, K, 4)).transpose((1, 0, 2))\n",
    "    anchor = anchor.reshape((K * A, 4)).astype(np.float32)\n",
    "    return anchor # (22500, 4) -> (anchor box Ïàò, (y1, x1, y2, x2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a50c2e-fb65-4c93-b9b8-a5bcf99c6b9f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### 1-2-1-2-1) gnerate_anchor_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e1f8274-5f96-4e35-b5ac-d88caf09554d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_anchor_base(base_size=16, ratios=[0.5, 1, 2], anchor_scales=[8, 16, 32]):\n",
    "    \"\"\" \n",
    "    ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄ Í∏∞Ï§Ä base_size = 16\n",
    "    Ìïú ÌîΩÏÖÄÎãπ Î™áÍ∞úÏùò anchor boxÍ∞Ä ÏÉùÏÑ± ÎêòÎÇò?\n",
    "    ÌïòÎÇòÏùò ÌîΩÏÖÄÏóê ÎåÄÌï¥ÏÑú ÏúÑÏπò Ï†ïÎ≥¥ ÏóÜÏù¥ ÌÅ¨Í∏∞Îßå return\n",
    "    Args:\n",
    "        ratios: ÎπÑÏú®\n",
    "        anchor_scales: Ïä§ÏºÄÏùº\n",
    "    Returns: basic anchor boxes, shape=(R, 4)\n",
    "        R: len(ratio) * len(anchor_scales) = anchor Í∞úÏàò = 9\n",
    "        4: anchor box Ï¢åÌëú Í∞í\n",
    "    \"\"\"\n",
    "    \n",
    "    py = base_size / 2. # center y\n",
    "    px = base_size / 2. # center x\n",
    "\n",
    "    # (9 x 4)\n",
    "    anchor_base = np.zeros((len(ratios) * len(anchor_scales), 4), dtype=np.float32) # Îπà achor box anchor_box\n",
    "    \n",
    "    for i in range(len(ratios)):\n",
    "        for j in range(len(anchor_scales)):\n",
    "            \n",
    "            # HW = S^2 ÎùºÍ≥† Ï†ïÏùòÌñáÏùå ÎÖºÎ¨∏Ìï¥ÏÑú\n",
    "            # H = rW ÎùºÍ≥† Ï†ïÏùòÌñàÏùå ÎÖºÎ¨∏ÏóêÏÑú\n",
    "            # rW^2 = S^2\n",
    "            # W^2 = S^2 / r\n",
    "            # w = s * sqrt(1/ r)\n",
    "            \n",
    "            h = base_size * anchor_scales[j] * np.sqrt(ratios[i])       # height\n",
    "            w = base_size * anchor_scales[j] * np.sqrt(1. / ratios[i])  # width\n",
    "\n",
    "            index = i * len(anchor_scales) + j\n",
    "            \n",
    "            # offset of anchor box\n",
    "            anchor_base[index, 0] =  py - h / 2 # y_min\n",
    "            anchor_base[index, 1] =  px - w / 2 # x_min\n",
    "            anchor_base[index, 2] =  py + h / 2 # y_max\n",
    "            anchor_base[index, 3] =  px + w / 2 # x_max\n",
    "            \n",
    "    return anchor_base # (9,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de143dca-eaba-4ffa-81e5-226f348b2e51",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### 1-2-1-2-2) Proposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d65757f2-db5c-4ca9-9f66-656045f586ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProposalCreator:\n",
    "    \"\"\"\n",
    "    RPNÏùò class prediction(score)Ï†ïÎ≥¥ÏôÄ box prediction(Ï°∞Ï†ï) Ï†ïÎ≥¥Î•º Ïù¥Ïö©Ìï¥ÏÑú fast rcnnÏóê ÎÑòÍ∏∏ roiÎ•º ÎÑòÍ≤®Ï§ÄÎã§.\n",
    "    \"\"\"\n",
    "    def __init__(self, parent_model,\n",
    "                 nms_thresh=0.7, # nms threshold\n",
    "                 n_train_pre_nms=12000, # trainÏãú nms Ï†Ñ roi Í∞úÏàò - scoreÎ°ú Ïö∞ÏÑ† Ïù¥Ï†ïÎèÑ ÌïÑÌÑ∞ÌïúÎã§.\n",
    "                 n_train_post_nms=2000, # trainÏãú nms ÌõÑ roi Í∞úÏàò - nmsÎ•º Í±∞ÏπòÍ≥† ÌïúÎ≤àÎçî ÌïÑÌÑ∞\n",
    "                 n_test_pre_nms=6000,   # testÏãú nms Ï†Ñ roi Í∞úÏàò\n",
    "                 n_test_post_nms=300,   # testÏãú nms ÌõÑ roi Í∞úÏàò\n",
    "                 min_size=16            \n",
    "                 ):\n",
    "        self.parent_model = parent_model # Ìï¥Îãπ Î™®Îç∏Ïù¥ trainÏ§ëÏù∏ÏßÄ testÏ§ëÏù∏ÏßÄ ÎÇòÌÉÄÎÉÑ\n",
    "        self.nms_thresh = nms_thresh\n",
    "        self.n_train_pre_nms = n_train_pre_nms\n",
    "        self.n_train_post_nms = n_train_post_nms\n",
    "        self.n_test_pre_nms = n_test_pre_nms\n",
    "        self.n_test_post_nms = n_test_post_nms\n",
    "        self.min_size = min_size # ÏµúÏÜå bboxes ÌÅ¨Í∏∞\n",
    "\n",
    "    def __call__(self, loc, score, anchor, img_size, scale=1.):   \n",
    "        # (22500, 4) loc Ï†ïÎ≥¥, (22500,)Ïùò  fg score, (22500, 4) anchore box, ÏõêÎ≥∏Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞, scale Í∞í = 1\n",
    "        if self.parent_model.training: # trainÏ§ëÏùº Îïå\n",
    "            n_pre_nms = self.n_train_pre_nms\n",
    "            n_post_nms = self.n_train_post_nms\n",
    "        else: # testÏ§ëÏùº Îïå\n",
    "            n_pre_nms = self.n_test_pre_nms\n",
    "            n_post_nms = self.n_test_post_nms\n",
    "\n",
    "        # anchorÏùò Ï¢åÌëúÍ∞íÍ≥º predicted bounding bounding box offset(y,x,h,w)Î•º ÌÜµÌï¥ bounding box Ï¢åÌëúÍ∞í(y_min, x_min, y_max, x_max) ÏÉùÏÑ±\n",
    "        roi = loc2bbox(anchor, loc) # (22500, 4), (22500, 4) -> (22500, 4)\n",
    "\n",
    "        # img_sizeÏù∏ Ïù¥Ïú† feet_strideÎûë Ïó∞Í¥Ä\n",
    "        # Clip predicted boxes to image.\n",
    "        roi[:, slice(0, 4, 2)] = np.clip(roi[:, slice(0, 4, 2)], 0, img_size[0])\n",
    "        roi[:, slice(1, 4, 2)] = np.clip(roi[:, slice(1, 4, 2)], 0, img_size[1])\n",
    "\n",
    "        # min_size Î≥¥Îã§ ÏûëÏùÄ boxÎì§ÏùÄ Ï†úÍ±∞\n",
    "        min_size = self.min_size * scale\n",
    "        hs = roi[:, 2] - roi[:, 0]\n",
    "        ws = roi[:, 3] - roi[:, 1]\n",
    "        \n",
    "        # 22500 -> 22500 > ?(18000)\n",
    "        keep = np.where((hs >= min_size) & (ws >= min_size))[0]\n",
    "        roi = roi[keep, :] # (18000, 4)\n",
    "        score = score[keep] # (18000,)\n",
    "        \n",
    "        # Sort all (proposal, score) pairs by score from highest to lowest.\n",
    "        # Take top pre_nms_topN \n",
    "        order = score.ravel().argsort()[::-1] # ÎÇ¥Î¶ºÏ∞®Ïàú Ï†ïÎ†¨ indexÎ•º Ï†ïÎ≥¥\n",
    "        if n_pre_nms > 0:\n",
    "            order = order[:n_pre_nms]\n",
    "        roi = roi[order, :] # (12000, 4)\n",
    "        score = score[order] # (12000,)\n",
    "\n",
    "        # nms Ï†ÅÏö© # (x<12000,) index \n",
    "        keep = nms(\n",
    "            torch.from_numpy(roi).cuda(),\n",
    "            torch.from_numpy(score).cuda(),\n",
    "            self.nms_thresh)\n",
    "        \n",
    "        if n_post_nms > 0:\n",
    "            keep = keep[:n_post_nms]\n",
    "        roi = roi[keep.cpu().numpy()] # (2000, 4)\n",
    "        \n",
    "        # ÏµúÏ¢ÖÏ†ÅÏúºÎ°ú 2Ï≤úÍ∞úÎßå ÏÇ¨Ïö©ÌïúÎã§.\n",
    "        return roi "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b780414-cf61-432b-92bb-f080307e705a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1-2-1-3) head(RoI VGG16RoIHead)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a88894-3b45-4c3f-a5f3-9f91e8902a15",
   "metadata": {},
   "source": [
    "RPNÏóêÏÑú ÎÇòÏò® 2000Í∞ú RoIÎ°ú Î∂ÄÌÑ∞ RoI poolÏùÑ Ìïú ÌõÑÏóê classifier, regressor ÌÜµÍ≥º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14bc1362-f79e-4282-90f7-c3f96417dd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16RoIHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Faster R-CNN head\n",
    "    RoI pool ÌõÑÏóê classifier, regressior ÌÜµÍ≥º\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_class, roi_size, spatial_scale, classifier):\n",
    "        \"\"\" \n",
    "        Args:\n",
    "            n_class: Î∞∞Í≤Ω Ìè¨Ìï® class Ïàò\n",
    "            roi_size: 7\n",
    "            spatial_scale: (1. / feat_stride),\n",
    "            classifier: vgg classifier\n",
    "        \"\"\"\n",
    "        super(VGG16RoIHead, self).__init__()\n",
    "\n",
    "        self.classifier = classifier  \n",
    "        \n",
    "        # üíö 4096?\n",
    "        self.cls_loc = nn.Linear(4096, n_class * 4) # bounding box regressor\n",
    "        self.score = nn.Linear(4096, n_class) # Classifier\n",
    "\n",
    "        normal_init(self.cls_loc, 0, 0.001)  # weight initialize\n",
    "        normal_init(self.score, 0, 0.01)     # weight initialize\n",
    "\n",
    "        self.n_class = n_class # Î∞∞Í≤Ω Ìè¨Ìï®Ìïú class Ïàò\n",
    "        self.roi_size = roi_size # RoI-pooling ÌõÑ feature mapÏùò  ÎÜíÏù¥, ÎÑàÎπÑ\n",
    "        self.spatial_scale = spatial_scale # roi resize scale\n",
    "        self.roi = RoIPool(output_size=(self.roi_size, self.roi_size), spatial_scale=self.spatial_scale)\n",
    "\n",
    "    def forward(self, x, rois, roi_indices):\n",
    "        # in case roi_indices is  ndarray\n",
    "        roi_indices = totensor(roi_indices).float()\n",
    "        rois = totensor(rois).float()\n",
    "        indices_and_rois = torch.cat([roi_indices[:, None], rois], dim=1)\n",
    "        # NOTE: important: yx->xy\n",
    "        xy_indices_and_rois = indices_and_rois[:, [0, 2, 1, 4, 3]]\n",
    "        indices_and_rois =  xy_indices_and_rois.contiguous() \n",
    "\n",
    "        # Í∞Å Ïù¥ÎØ∏ÏßÄ roi pooling \n",
    "        pool = self.roi(x, indices_and_rois) \n",
    "        # flatten \n",
    "        pool = pool.view(pool.size(0), -1)\n",
    "        # fully connected\n",
    "        fc7 = self.classifier(pool)\n",
    "        # regression \n",
    "        roi_cls_locs = self.cls_loc(fc7)\n",
    "        # softmax\n",
    "        roi_scores = self.score(fc7)\n",
    "        \n",
    "        return roi_cls_locs, roi_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88076c5-b04d-40e7-8c94-8ca6f8c89885",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 1-2-1-4) FasterRCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "141f16a5-eefd-4b96-b1c3-cdb187e1e73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FasterRCNN(nn.Module):\n",
    "    def __init__(self, extractor, rpn, head,\n",
    "                loc_normalize_mean = (0., 0., 0., 0.),\n",
    "                loc_normalize_std = (0.1, 0.1, 0.2, 0.2)):\n",
    "        super(FasterRCNN, self).__init__()\n",
    "        self.extractor = extractor  # extractor : vgg\n",
    "        self.rpn = rpn              # rpn : region proposal network\n",
    "        self.head = head            # head : RoiHead\n",
    "\n",
    "        # mean and std\n",
    "        self.loc_normalize_mean = loc_normalize_mean\n",
    "        self.loc_normalize_std = loc_normalize_std\n",
    "        self.use_preset()\n",
    "\n",
    "    @property\n",
    "    def n_class(self): # ÏµúÏ¢Ö class Í∞úÏàò (Î∞∞Í≤Ω Ìè¨Ìï®)\n",
    "        return self.head.n_class\n",
    "\n",
    "    # üíöüíôüíõüß°üíú\n",
    "    # predict Ïãú ÏÇ¨Ïö©ÌïòÎäî forward\n",
    "    # train Ïãú FasterRCNNTrainerÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ FasterRcnnÏóê ÏûàÎäî extractor, rpn, headÎ•º Î™®ÎìàÎ≥ÑÎ°ú Î∂àÎü¨ÏôÄÏÑú forward\n",
    "    def forward(self, x, scale=1.):\n",
    "        img_size = x.shape[2:]\n",
    "\n",
    "        h = self.extractor(x) # extractor ÌÜµÍ≥º\n",
    "        rpn_locs, rpn_scores, rois, roi_indices, anchor = self.rpn(h, img_size, scale) # rpn ÌÜµÍ≥º\n",
    "        roi_cls_locs, roi_scores = self.head(h, rois, roi_indices) # head ÌÜµÍ≥º\n",
    "        return roi_cls_locs, roi_scores, rois, roi_indices \n",
    "\n",
    "    def use_preset(self): # prediction Í≥ºÏ†ï Ïì∞Ïù¥Îäî threshold Ï†ïÏùò\n",
    "        self.nms_thresh = 0.3\n",
    "        self.score_thresh = 0.05\n",
    "\n",
    "    def _suppress(self, raw_cls_bbox, raw_prob):\n",
    "        bbox = list()\n",
    "        label = list()\n",
    "        score = list()\n",
    "        \n",
    "        # skip cls_id = 0 because it is the background class\n",
    "        for l in range(1, self.n_class):\n",
    "            cls_bbox_l = raw_cls_bbox.reshape((-1, self.n_class, 4))[:, l, :]\n",
    "            prob_l = raw_prob[:, l]\n",
    "            mask = prob_l > self.score_thresh\n",
    "            cls_bbox_l = cls_bbox_l[mask]\n",
    "            prob_l = prob_l[mask]\n",
    "            keep = nms(cls_bbox_l, prob_l,self.nms_thresh)\n",
    "            bbox.append(cls_bbox_l[keep].cpu().numpy())\n",
    "            # The labels are in [0, self.n_class - 2].\n",
    "            label.append((l - 1) * np.ones((len(keep),)))\n",
    "            score.append(prob_l[keep].cpu().numpy())\n",
    "        \n",
    "        bbox = np.concatenate(bbox, axis=0).astype(np.float32)\n",
    "        label = np.concatenate(label, axis=0).astype(np.int32)\n",
    "        score = np.concatenate(score, axis=0).astype(np.float32)\n",
    "        return bbox, label, score\n",
    "\n",
    "    @nograd\n",
    "    def predict(self, imgs,sizes=None):\n",
    "        \"\"\"\n",
    "        Ïù¥ÎØ∏ÏßÄÏóêÏÑú Í∞ùÏ≤¥ Í≤ÄÏ∂ú\n",
    "        Input : images\n",
    "        Output : bboxes, labels, scores\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        prepared_imgs = imgs\n",
    "                \n",
    "        bboxes = list()\n",
    "        labels = list()\n",
    "        scores = list()\n",
    "        for img, size in zip(prepared_imgs, sizes):\n",
    "            img = totensor(img[None]).float()\n",
    "            scale = img.shape[3] / size[1]\n",
    "            roi_cls_loc, roi_scores, rois, _ = self(img, scale=scale) # self = FasterRCNN\n",
    "            # We are assuming that batch size is 1.\n",
    "            roi_score = roi_scores.data\n",
    "            roi_cls_loc = roi_cls_loc.data\n",
    "            roi = totensor(rois) / scale\n",
    "\n",
    "            # Convert predictions to bounding boxes in image coordinates.\n",
    "            # Bounding boxes are scaled to the scale of the input images.\n",
    "            mean = torch.Tensor(self.loc_normalize_mean).cuda(). repeat(self.n_class)[None]\n",
    "            std = torch.Tensor(self.loc_normalize_std).cuda(). repeat(self.n_class)[None]\n",
    "\n",
    "            roi_cls_loc = (roi_cls_loc * std + mean)\n",
    "            roi_cls_loc = roi_cls_loc.view(-1, self.n_class, 4)\n",
    "            roi = roi.view(-1, 1, 4).expand_as(roi_cls_loc)\n",
    "            cls_bbox = loc2bbox(tonumpy(roi).reshape((-1, 4)),tonumpy(roi_cls_loc).reshape((-1, 4)))\n",
    "            cls_bbox = totensor(cls_bbox)\n",
    "            cls_bbox = cls_bbox.view(-1, self.n_class * 4)\n",
    "            # clip bounding box\n",
    "            cls_bbox[:, 0::2] = (cls_bbox[:, 0::2]).clamp(min=0, max=size[0])\n",
    "            cls_bbox[:, 1::2] = (cls_bbox[:, 1::2]).clamp(min=0, max=size[1])\n",
    "\n",
    "            prob = (F.softmax(totensor(roi_score), dim=1))\n",
    "\n",
    "            bbox, label, score = self._suppress(cls_bbox, prob)\n",
    "            bboxes.append(bbox)\n",
    "            labels.append(label)\n",
    "            scores.append(score)\n",
    "\n",
    "        self.use_preset()\n",
    "        self.train()\n",
    "        return bboxes, labels, scores\n",
    "\n",
    "    def get_optimizer(self):\n",
    "        '''\n",
    "        Optimizer ÏÑ†Ïñ∏\n",
    "        '''\n",
    "        lr = learning_rate\n",
    "        params = []\n",
    "        for key, value in dict(self.named_parameters()).items():\n",
    "            if value.requires_grad:\n",
    "                if 'bias' in key:\n",
    "                    params += [{'params': [value], 'lr': lr * 2, 'weight_decay': 0}]\n",
    "                else:\n",
    "                    params += [{'params': [value], 'lr': lr, 'weight_decay': weight_decay}]\n",
    "        self.optimizer = torch.optim.SGD(params, momentum=0.9)\n",
    "        return self.optimizer\n",
    "\n",
    "    def scale_lr(self, decay=0.1):\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] *= decay\n",
    "        return self.optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd92419e-bbb3-4838-b93f-8e93de93bb9f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1-2-2) AnchortargetCreator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d946ca77-067b-411e-afd6-4821c862ec3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnchorTargetCreator(object):\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_sample=256,\n",
    "                 pos_iou_thresh=0.7, neg_iou_thresh=0.3,\n",
    "                 pos_ratio=0.5):\n",
    "        self.n_sample = n_sample\n",
    "        self.pos_iou_thresh = pos_iou_thresh\n",
    "        self.neg_iou_thresh = neg_iou_thresh\n",
    "        self.pos_ratio = pos_ratio\n",
    "\n",
    "    def __call__(self, bbox, anchor, img_size):\n",
    "\n",
    "        img_H, img_W = img_size\n",
    "\n",
    "        n_anchor = len(anchor) # 9216\n",
    "        inside_index = get_inside_index(anchor, img_H, img_W) # (2272,)\n",
    "        anchor = anchor[inside_index] # (2272, 4)\n",
    "        argmax_ious, label = self._create_label(\n",
    "            inside_index, anchor, bbox)\n",
    "\n",
    "        # compute bounding box regression targets\n",
    "        loc = bbox2loc(anchor, bbox[argmax_ious]) # (2272, 4)\n",
    "\n",
    "        # map up to original set of anchors\n",
    "        label = unmap(label, n_anchor, inside_index, fill=-1) # (9216,)\n",
    "        loc = unmap(loc, n_anchor, inside_index, fill=0) # (9216, 4)\n",
    "\n",
    "        return loc, label\n",
    "\n",
    "    def _create_label(self, inside_index, anchor, bbox):\n",
    "        # label) 1 :positive, 0 : negative, -1 : dont care\n",
    "        label = np.empty((len(inside_index),), dtype=np.int32)\n",
    "        label.fill(-1)\n",
    "\n",
    "        argmax_ious, max_ious, gt_argmax_ious = self._calc_ious(anchor, bbox, inside_index)\n",
    "\n",
    "        label[max_ious < self.neg_iou_thresh] = 0 # 0.3\n",
    "\n",
    "        # Í∞ÄÏû• iouÍ∞Ä ÌÅ∞ Í≤ÉÏùÄ positive label\n",
    "        label[gt_argmax_ious] = 1\n",
    "\n",
    "        # positive label\n",
    "        label[max_ious >= self.pos_iou_thresh] = 1 # 0.7\n",
    "\n",
    "        # subsample positive labels if we have too many\n",
    "        n_pos = int(self.pos_ratio * self.n_sample)\n",
    "        pos_index = np.where(label == 1)[0]\n",
    "        if len(pos_index) > n_pos:\n",
    "            disable_index = np.random.choice(\n",
    "                pos_index, size=(len(pos_index) - n_pos), replace=False)\n",
    "            label[disable_index] = -1\n",
    "\n",
    "        # subsample negative labels if we have too many\n",
    "        n_neg = self.n_sample - np.sum(label == 1)\n",
    "        neg_index = np.where(label == 0)[0]\n",
    "        if len(neg_index) > n_neg:\n",
    "            disable_index = np.random.choice(\n",
    "                neg_index, size=(len(neg_index) - n_neg), replace=False)\n",
    "            label[disable_index] = -1\n",
    "\n",
    "        return argmax_ious, label\n",
    "\n",
    "    def _calc_ious(self, anchor, bbox, inside_index):\n",
    "        # ious between the anchors and the gt boxes\n",
    "        ious = bbox_iou(anchor, bbox)\n",
    "        argmax_ious = ious.argmax(axis=1)\n",
    "        max_ious = ious[np.arange(len(inside_index)), argmax_ious]\n",
    "        gt_argmax_ious = ious.argmax(axis=0)\n",
    "        gt_max_ious = ious[gt_argmax_ious, np.arange(ious.shape[1])]\n",
    "        gt_argmax_ious = np.where(ious == gt_max_ious)[0]\n",
    "\n",
    "        return argmax_ious, max_ious, gt_argmax_ious"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c46e8c-5843-41b0-8b1d-3d5365b57d64",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1-2-3) ProposalTargetCreator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f41bf7c-8a70-4c5e-b6eb-eabd39691271",
   "metadata": {},
   "source": [
    "RPNÏóêÏÑú NMSÎ•º Í±∞Ïπú roiÎì§ÏùÑ ground truthÏôÄÏùò iouÎ•º ÎπÑÍµê  \n",
    "positive / negative sampling ÏàòÌñâ (Ï¥ù 128Í∞ú)  \n",
    "sample roiÏôÄ gt_bboxÎ•º Ïù¥Ïö©Ìï¥ bbox regressionÏóêÏÑú regressionÌï¥ÏïºÌï† ground truth locÍ∞í(t_x, t_y, t_w, t_h)ÏùÑ Íµ¨Ìï®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4bce95d1-eefc-4d22-9cc9-89657d0eee8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProposalTargetCreator:\n",
    "    def __init__(self,\n",
    "                 n_sample=128,\n",
    "                 pos_ratio=0.25, pos_iou_thresh=0.5,\n",
    "                 neg_iou_thresh_hi=0.5, neg_iou_thresh_lo=0.0\n",
    "                 ):\n",
    "        self.n_sample = n_sample\n",
    "        self.pos_ratio = pos_ratio\n",
    "        self.pos_iou_thresh = pos_iou_thresh # positive iou threshold\n",
    "        self.neg_iou_thresh_hi = neg_iou_thresh_hi # negitave iou threshold = (neg_iou_thresh_hi ~ neg_iou_thresh_lo)\n",
    "        self.neg_iou_thresh_lo = neg_iou_thresh_lo \n",
    "\n",
    "    def __call__(self, roi, bbox, label,\n",
    "                 loc_normalize_mean=(0., 0., 0., 0.),\n",
    "                 loc_normalize_std=(0.1, 0.1, 0.2, 0.2)):\n",
    "        \"\"\"\n",
    "        # input : (2000, 4), (boxÏàò, 4), (boxÏàò, 1), (0., 0., 0., 0.), (0.1, 0.1, 0.2, 0.2))\n",
    "        \"\"\"\n",
    "        n_bbox, _ = bbox.shape\n",
    "\n",
    "        # roi = np.concatenate((roi, bbox), axis=0)\n",
    "        \n",
    "        pos_roi_per_image = np.round(self.n_sample * self.pos_ratio) # positive image Í∞ØÏàò = 32\n",
    "        iou = bbox_iou(roi, bbox) # RoIÏôÄ bounding box IoU\n",
    "        \n",
    "        gt_assignment = iou.argmax(axis=1)\n",
    "        max_iou = iou.max(axis=1)\n",
    "        gt_roi_label = label[gt_assignment] + 1 # class label [0, n_fg_class - 1] -> [1, n_fg_class].\n",
    "\n",
    "        # positive sample ÏÑ†ÌÉù (>= pos_iou_thresh IoU)\n",
    "        pos_index = np.where(max_iou >= self.pos_iou_thresh)[0]\n",
    "        pos_roi_per_this_image = int(min(pos_roi_per_image, pos_index.size))\n",
    "        if pos_index.size > 0:\n",
    "            pos_index = np.random.choice(\n",
    "                pos_index, size=pos_roi_per_this_image, replace=False)\n",
    "\n",
    "        # Negative sample ÏÑ†ÌÉù [neg_iou_thresh_lo, neg_iou_thresh_hi)\n",
    "        neg_index = np.where((max_iou < self.neg_iou_thresh_hi) &\n",
    "                             (max_iou >= self.neg_iou_thresh_lo))[0]\n",
    "        neg_roi_per_this_image = self.n_sample - pos_roi_per_this_image\n",
    "        neg_roi_per_this_image = int(min(neg_roi_per_this_image,\n",
    "                                         neg_index.size))\n",
    "        if neg_index.size > 0:\n",
    "            neg_index = np.random.choice(\n",
    "                neg_index, size=neg_roi_per_this_image, replace=False)\n",
    "\n",
    "        # The indices that we're selecting (both positive and negative).\n",
    "        keep_index = np.append(pos_index, neg_index)\n",
    "        gt_roi_label = gt_roi_label[keep_index]\n",
    "        gt_roi_label[pos_roi_per_this_image:] = 0  # negative sampleÏùò label = 0\n",
    "        sample_roi = roi[keep_index] # (128, 4)\n",
    "\n",
    "        # sample roiÏôÄ gt_bboxÎ•º Ïù¥Ïö©Ìï¥ bbox regressionÏóêÏÑú regressionÌï¥ÏïºÌï† ground truth locÍ∞í(t_x, t_y, t_w, t_h) Í≥ÑÏÇ∞\n",
    "        gt_roi_loc = bbox2loc(sample_roi, bbox[gt_assignment[keep_index]]) # (128, 4)\n",
    "        gt_roi_loc = ((gt_roi_loc - np.array(loc_normalize_mean, np.float32)) / np.array(loc_normalize_std, np.float32))\n",
    "\n",
    "        return sample_roi, gt_roi_loc, gt_roi_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d53ad940-5bb1-4de8-ad26-3ef11ee71fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.19s)\n",
      "creating index...\n",
      "index created!\n",
      "--------------------------------------------------\n",
      "load data\n",
      "image shape : torch.Size([3, 800, 800])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4883 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2002, 2)\n",
      "(128, 4) (128, 4) (128,)\n",
      "check\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed099aeb-05bd-4341-83ad-2fce7fbdf83f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
